* Setup :ignore:

#+SETUPFILE: ~/public/hozen-style/latex/hozen.setup

# Ensure that we respect org mode new line
# #+OPTIONS: \n:t

# To disbale _ and ^ behaviour but keep ^{} and _{}
#+OPTIONS: ^:{}

#+LATEX_HEADER: \usepackage[linesnumbered]{algorithm2e}
#+LATEX_HEADER: \usepackage[inkscapelatex=false]{svg}

* Page de garde :ignore:
** Informations :ignore:

#+AUTHOR: Author: Enzo Durel
#+AUTHOR: \newline
#+AUTHOR: 
#+EMAIL: /
#+TITLE: 5043 Advanced Machine Learning - HW 6
#+OPTIONS: toc:nil

** Logo :ignore:

#+ATTR_LATEX: :width 10cm :align left
[[file:~/orgmode_latex_export_img/ou_logo.png]]

** newpage :noexport:

#+begin_export latex
\newpage
#+end_export

** Table des mati√®res :ignore:

#+LATEX: \thispagestyle{empty}
#+TOC: headlines 3
#+LATEX: \clearpage
#+LATEX: \pagenumbering{arabic} 

** Liste des figures :ignore:

#+begin_export latex
\thispagestyle{empty}
\listoffigures
\clearpage
\pagenumbering{arabic} 
#+end_export

** Liste des algorithmes :noexport:

#+begin_export latex
\thispagestyle{empty}
\listofalgorithms
\clearpage
\pagenumbering{arabic} 
#+end_export

** newpage :ignore:

#+begin_export latex
\newpage
#+end_export

* Figures
** Figure 0a

#+caption: RNN Model
#+attr_latex: :height 18cm :float nil
[[file:./img/figure_0a.png]]

** Figure 0b

#+caption: GRU Model
#+attr_latex: :height 18cm :float nil
[[file:./img/figure_0b.png]]

** Figure 0c

#+caption: Attention Model
#+attr_latex: :height 18cm :float nil
[[file:./img/figure_0c.png]]

** Figure 1a

#+caption: Training Loss
#+attr_latex: :width 12cm :float nil
[[file:./img/figure_1a.png]]

** Figure 1b

#+caption: Validation Loss
#+attr_latex: :width 12cm :float nil
[[file:./img/figure_1b.png]]

** Figure 2

#+caption: Validation accuracy GRU vs Attention
#+attr_latex: :width 12cm :float nil
[[file:./img/figure_2.png]]

** Figure 3

#+caption: Models Accuracy Across Rotations
#+attr_latex: :width 12cm :float nil
[[file:./img/figure_3.png]]

** Figure 4

#+caption: MHA Test Contingency Table across the five rotations
#+attr_latex: :width 12cm :float nil
[[file:./img/figure_4.png]]

** newpage :ignore:

#+begin_src latex
\newpage
#+end_src

* Analysis & Discussion

** Describe the specific choices you made in the model architectures.

*** RNN & GRU

The RNN and GRU follow the same architecture. Only the RNN part is replaced with GRU layers. First, we have the embeddings, then we have the convolution as we see in class.

About the RNN layers, I only use 1 inner RNN layer with the =return_sequence= parameter as false to keep dimensions. I designed my build model function as we can use as much RNN layers as we want. I firstly testing with one inner layer and it works great so I decided to keep this architecture.

We always have a "final" RNN layer to make the link with the hidden layers. Then we only have 2 dense layers. Same approach, I begin small and it works well so there was no need to scale up.

*** MHA

Concerning the MHA, I follow the same approach. First the embedding layer followed by the positional encoder. Then, the convolutional layer use to "skip" connections.

There is only one multi head attention layer which was working perfectly so I keep it small too. We've got the average pooling followed by the global pooling to "filter" the parameters.

Finally, we've got the same hidden layers as RNN & GRU. Only 2 small layers of size 64 and 32.

** Discuss in detail how consistent your model performance is across the different rotations.

We can see in figure 1a and 1b that all accuracy curves follow the same tendance independantly of the rotation.

However, we can see that the RNN layer validation accuracy curves are less stable than the GRU and MHA ones. We can also see that in the rotation 1, the GRU model is "lower" than the other GRU models.

MHA models are really stable and almost identical.

** Discuss the relative performance of the three model types.

In figure 2, we can that the MHA accuracy is getting quicly better but converge with the GRU accuracies. The GRU accuracy is getting converge slower but get almost the same accuracy at the end.

In figure 3, we can see that all the models have almost the same final accuracy independantly of the rotation. However, we can see that MHA is slightly better in every rotation.

** Compare and contrast the different models with respect to the required number of training epochs and the amount of time required to complete each epoch. Why does the third model type require so much compute time for each epoch? 

For each models, I used steps per epoch equal to 100.

For RNN layer, I've got in average, 37s per epoch.
For GRU layer, I've got in average, 14s per epoch.
For MHA layer, I've got in average, 10s per epoch.

Moreover, the GRU and RNN layers needs more epochs to get the almost the same accuracy of the MHA model which can only take a 20 of epochs.

In conlusion, it seems that the MHA model is better in every aspect compared to the simple RNN and GRU models. 


