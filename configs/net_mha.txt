--conv_size
3
--conv_nfilters
128
--conv_stride
4
--pool
1
--attention_heads
4
--attention_key_dim
128
--attention_pool
1
--hidden
128
--activation_conv=relu
--activation_dense=relu
--embedding_input_dim=128
--L2_regularization=.001
--dropout=.1
--batch_normalization
--lrate=.0001
--batch=128
--label=NET_MHA
--model_type=attention
